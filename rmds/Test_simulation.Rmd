---
title: "Simulate datasets"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "/n/janson_lab/lab/sma/sparsedossa_paper/")
knitr::opts_chunk$set(echo=FALSE)
library(magrittr)
library(ggplot2)
```

```{r setup2}
# Registry
# batchtools::makeRegistry(file.dir = "r_batchtools_reg/tmp/Test/",
#                          package = "magrittr")
batchtools::loadRegistry(file.dir = "r_batchtools_reg/tmp/Test/", 
                         writeable = TRUE)
batchtools::clearRegistry()

# Grid parameters
ncpus <- 1
partition <- "janson,janson_cascade,shared"
walltime <- 5 * 3600

dir_output <- "/n/janson_lab/lab/sma/sparsedossa_paper/results/tmp/Test_Simulation/"
dir.create(dir_output, recursive = TRUE)
```

```{r setup simulation params}
set.seed(1)
tb_job <- tidyr::expand_grid(
  method = c("metaSPARSim", "SparseDOSSA2"),
  dataset = c("stool"),
  K_split = seq(1, 5)
) %>% 
  dplyr::mutate(i_job = seq_len(dplyr::n())) %>% 
  dplyr::group_by(dataset, K_split) %>% 
  dplyr::mutate(training = {
    load(paste0("data/physeqs/", dataset[1], ".RData"))
    n_sample <- get(paste0("physeq_", dataset[1], "_bl")) %>% 
      phyloseq::nsamples()
    sample.int(n_sample, round(n_sample / 2)) %>% list()
  }) %>% 
  dplyr::ungroup()
save(tb_job, file = paste0(dir_output, "tb_job.RData"))
```

```{r define one job}
one_job <- function(i_job) {
  i_tb_job <- tb_job[i_job, ]
  
  load(paste0("data/physeqs/", i_tb_job$dataset, ".RData"))
  x_samples <- get(paste0("physeq_", i_tb_job$dataset, "_bl")) %>% 
  {phyloseq::prune_samples(seq_len(phyloseq::nsamples(.)) %in%
                             i_tb_job$training[[1]],
                           .)} %>% 
    smar::prune_taxaSamples() %>% 
    smar::otu_table2()
    
  if(i_tb_job$method == "SparseDOSSA2") {
    future::plan(future::multisession())
  
    fit <- SparseDOSSA2:::fit_SparseDOSSA2(
      data = x_samples,
      lambda = 1,
      control = list(abs_tol = 1e-2,
                     rel_tol = 1e-3,
                     maxit = 50,
                     verbose = TRUE,
                     debug_dir = paste0(dir_output, i_job, "/")))
  save(fit, file = paste0(dir_output, "fit_", i_job, ".RData"))
  }
    
  if(i_tb_job$method == "metaSPARSim") {
    source("R/GMPR.R")
    require(matrixStats)
    
    norm_factor <- GMPR(x_samples, intersect.no = 1)
    x_samples_norm <- t(t(x_samples) / norm_factor)
    fit <- metaSPARSim::estimate_parameter_from_data(
      raw_data = x_samples,
      norm_data = x_samples_norm,
      conditions = list("group" = colnames(x_samples)),
      perc_not_zeros = 0)
    save(fit, file = paste0(dir_output, "fit_", i_job, ".RData"))
  }
  # 
  # if(i_tb_job$method == "DMM") {
  #   sim_DMM <- MCMCpack::rdirichlet(n = n_sample,
  #                                   alpha = fit_DMM$gamma) %>% 
  #   t()
  #   save(sim_DMM, file = paste0(dir_output, "DMM_", i_job, ".RData"))
  #   sim_data <- vapply(seq_len(n_sample),
  #                      function(i_sample) {
  #                        rmultinom(n = 1,
  #                                  size = lib_size[i_sample],
  #                                  prob = sim_DMM[, i_sample])
  #                      },
  #                      rep(0, nrow(sim_DMM)))
  # }
  # 
  # dimnames(sim_data) <- list(rownames(x_samples),
  #                            paste0("Sample", seq(1, n_sample)))
  # 
  # save(sim_data, file = paste0(dir_output, i_job, ".RData"))
}
```

```{r submit jobs}
tb_ids <- batchtools::batchMap(one_job,
                               i_job = tb_job$i_job)
batchtools::batchExport(mget(ls()))
batchtools::submitJobs(ids = tb_ids$job.id,
                       resources =  list(ncpus = ncpus, 
                                         partition = partition, 
                                         walltime = walltime))
```